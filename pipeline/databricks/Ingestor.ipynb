{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a8fc79-10bd-4e92-8b24-87e6410b5b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64668e2-2ed6-4eb7-a77f-d7e21889cbb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "class Ingestor(object):\n",
    "    def __init__(self, table_set=\"none\"):\n",
    "        self.storage_account = \"northwinddl\"\n",
    "        self.external_location = \"prod\"\n",
    "        self.catalog = \"northwind\"\n",
    "        self.container_path = f\"abfss://{self.external_location}@{self.storage_account}.dfs.core.windows.net\"\n",
    "        self.raw_path = f\"{self.container_path}/raw\"\n",
    "        self.bronze_path = f\"{self.container_path}/bronze\"\n",
    "        self.silver_path = f\"{self.container_path}/silver\"\n",
    "        self.gold_path = f\"{self.container_path}/gold\"\n",
    "        self.table_set = table_set\n",
    "        self.config = self.read_config()\n",
    "        self.processing_columns = {\n",
    "            \"_file_path\": \"STRING COMMENT 'The path from which the data was ingested'\",\n",
    "            \"_file_name\": \"STRING COMMENT 'The name of the file from which the data was ingested'\",\n",
    "            \"_file_size\": \"BIGINT COMMENT 'The size of the file from which the data was ingested'\",\n",
    "            \"_file_modification_time\": \"TIMESTAMP COMMENT 'The last modified timestamp of the file from which the data was ingested'\",\n",
    "            \"_loaded_at_utc\": \"TIMESTAMP COMMENT 'The timestamp at which the data was ingested'\",\n",
    "            \"_last_updated_at_utc\": \"TIMESTAMP COMMENT 'The timestamp at which the data was last updated'\"\n",
    "        }\n",
    "        self.client_id = dbutils.secrets.get(scope='northwind-scope', key='northwinddl-client-id')\n",
    "        self.tenant_id = dbutils.secrets.get(scope='northwind-scope', key='northwinddl-tenant-id')\n",
    "        self.client_secret = dbutils.secrets.get(scope='northwind-scope', key='northwinddl-client-secret')\n",
    "        self.timer_start = None\n",
    "        self.timer_end = None\n",
    "\n",
    "    def ts_print(self, message):\n",
    "        '''\n",
    "        Prints the input message with the timestamp preprended\n",
    "        '''\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{now}] >>> {message}\")\n",
    "\n",
    "    def start_timer(self):\n",
    "        '''\n",
    "        Starts the timer to measure the duration of the job.\n",
    "        '''\n",
    "        self.timer_start = datetime.now()\n",
    "\n",
    "    def stop_timer(self):\n",
    "        '''\n",
    "        Stops the timer to measure the duration of the job.\n",
    "        '''\n",
    "        self.timer_end = datetime.now()\n",
    "\n",
    "    def calculate_duration(self):\n",
    "        '''\n",
    "        Calculates the duration of the job.\n",
    "        '''\n",
    "        if self.timer_start is None or self.timer_end is None:\n",
    "            return None\n",
    "        else:\n",
    "            duration = (self.timer_end - self.timer_start).total_seconds()\n",
    "            hours = math.floor(duration / 60 / 60)\n",
    "            mins = math.floor((duration - (hours * 60 * 60)) / 60)\n",
    "            seconds = math.floor(duration % 60)\n",
    "            return f\"Total duration: {hours}h {mins}m {seconds}s\"\n",
    "\n",
    "    def overwrite_config(self):\n",
    "        '''\n",
    "        Reloads the configuration tables from the silver container to ensure they are up-to-date during table processing.\n",
    "        '''\n",
    "        # Overwrite table configuration\n",
    "        df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(f'{self.silver_path}/config/config_tables.csv')\n",
    "        df.write.format('delta').mode('overwrite').option('overwriteSchema', True).save(f'{self.silver_path}/config/config_tables')\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.silver.config_tables\n",
    "            USING DELTA\n",
    "            LOCATION '{self.silver_path}/config/config_tables'\n",
    "        \"\"\"\n",
    "        spark.sql(sql_query)\n",
    "\n",
    "        self.ts_print(\"config_tables successfully overwritten.\")\n",
    "\n",
    "        # Overwrite field configuration\n",
    "        df = spark.read.format('csv').option('header', True).option('inferSchema', True).load(f'{self.silver_path}/config/config_fields.csv')\n",
    "        df.write.format('delta').mode('overwrite').option('overwriteSchema', True).save(f'{self.silver_path}/config/config_fields')\n",
    "        sql_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.silver.config_fields\n",
    "            USING DELTA\n",
    "            LOCATION '{self.silver_path}/config/config_fields'        \n",
    "        \"\"\"\n",
    "        spark.sql(sql_query)\n",
    "\n",
    "        self.ts_print(\"config_fields successfully overwritten.\")\n",
    "\n",
    "    def read_config(self):\n",
    "        '''\n",
    "        Loads the table and column configurations into a dataframe.\n",
    "        '''\n",
    "        configs = spark \\\n",
    "            .sql(\"SHOW TABLES IN northwind.silver\") \\\n",
    "            .filter(\"tableName like 'config_%'\")\n",
    "\n",
    "        # If both config tables aren't loaded to silver, then load them from the csvs in the silver directory\n",
    "        if configs.count() < 2:\n",
    "            self.overwrite_config()\n",
    "\n",
    "        sql_query = f\"\"\"\n",
    "            SELECT\n",
    "                *,\n",
    "                CASE\n",
    "                    WHEN array_contains(split(primary_keys, \";\"), CASE WHEN layer = 'bronze' THEN f.field_raw ELSE f.field_silver END) THEN 'Y'\n",
    "                    ELSE 'N'\n",
    "                END AS is_primary_key\n",
    "            FROM\n",
    "                {self.catalog}.silver.config_tables AS t\n",
    "                INNER JOIN {self.catalog}.silver.config_fields AS f USING (table_set)\n",
    "            WHERE\n",
    "                table_set = '{self.table_set}'        \n",
    "        \"\"\"\n",
    "\n",
    "        return spark.sql(sql_query)\n",
    "\n",
    "    def table_exists(self, layer, table):\n",
    "        '''\n",
    "        Checks if the specified table exists in the specified schema\n",
    "        '''\n",
    "        sql_query = f\"SHOW TABLES IN northwind.{layer}\"\n",
    "        df = spark \\\n",
    "            .sql(sql_query) \\\n",
    "            .filter(f\"tableName == '{table}'\")\n",
    "\n",
    "        if df.count() == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def data_type_lookup(self, data_type):\n",
    "        '''\n",
    "        Retrieves the corresponding spark data type when instantiating the dataframe.\n",
    "        '''\n",
    "        data_types = {\n",
    "            'DECIMAL': FloatType(),\n",
    "            'FLOAT': FloatType(),\n",
    "            'INT': IntegerType(),\n",
    "            'LONG': IntegerType(),\n",
    "            'STRING': StringType(),\n",
    "            'TIMESTAMP': TimestampType()\n",
    "        }\n",
    "        return data_types.get(data_type)\n",
    "\n",
    "    def is_nullable_lookup(self, primary_keys, field):\n",
    "        '''\n",
    "        Determines if the column is nullable when instantiating the dataframe.\n",
    "        '''\n",
    "        if primary_keys is None:\n",
    "            return True\n",
    "        else:\n",
    "            return False if field in primary_keys.split(';') else True\n",
    "\n",
    "    def get_schema(self, layer, load_type):\n",
    "        '''\n",
    "        Gets the schema of the table being processed when instantiating the dataframe.\n",
    "        '''\n",
    "        schema = StructType()\n",
    "\n",
    "        for row in self.config.filter(f\"layer = '{layer}'\").collect():\n",
    "\n",
    "            primary_keys = row['primary_keys']\n",
    "            field = row['field_raw'] if layer == 'bronze' else row['field_silver']\n",
    "            comment = row['field_description']\n",
    "\n",
    "            schema.add(\n",
    "                field,                                          # Field name\n",
    "                self.data_type_lookup(row['type_silver']),      # Data type\n",
    "                self.is_nullable_lookup(primary_keys, field),   # Is nullable\n",
    "                {\"comment\": comment}                            # Comment\n",
    "            )\n",
    "\n",
    "        return schema\n",
    "\n",
    "    def create_table(self, layer, table, table_desc, path, tbl_properties, config, load_type):\n",
    "        '''\n",
    "        Creates the table in the target schema from the specified external location.\n",
    "        '''\n",
    "        self.ts_print(f\"Creating table `{self.catalog}.{layer}.{table}`...\")\n",
    "\n",
    "        schema = []\n",
    "        for row in config:\n",
    "            field = row[\"field_raw\" if layer == \"bronze\" else \"field_silver\"]\n",
    "            data_type = row[\"type_silver\"]\n",
    "            comment = row[\"field_description\"]\n",
    "            schema.append(f\"{field} {data_type} COMMENT '{comment}'\")\n",
    "\n",
    "        if layer == \"bronze\":\n",
    "            schema.append(f\"_file_path {self.processing_columns.get('_file_path')}\")\n",
    "            schema.append(f\"_file_name {self.processing_columns.get('_file_name')}\")\n",
    "            schema.append(f\"_file_size {self.processing_columns.get('_file_size')}\")\n",
    "            schema.append(f\"_file_modification_time {self.processing_columns.get('_file_modification_time')}\")\n",
    "            schema.append(f\"_loaded_at_utc {self.processing_columns.get('_loaded_at_utc')}\")\n",
    "\n",
    "        if layer == \"bronze\" and \"STREAM\" in load_type:\n",
    "            schema.append(f\"_last_updated_at_utc {self.processing_columns.get('_last_updated_at_utc')}\")\n",
    "\n",
    "        if layer == \"silver\" and \"STREAM\" in load_type:\n",
    "            schema.append(f\"_loaded_at_utc {self.processing_columns.get('_loaded_at_utc')}\")\n",
    "            schema.append(f\"_last_updated_at_utc {self.processing_columns.get('_last_updated_at_utc')}\")\n",
    "\n",
    "        schema = ','.join(schema)\n",
    "        schema = f\"({schema})\"\n",
    "\n",
    "        tbl_properties = \",\".join([f\"'{k}'='{v}'\" for k, v in tbl_properties.items()])\n",
    "\n",
    "        sql_query = (f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{layer}.{table} {schema}\n",
    "            USING DELTA\n",
    "            COMMENT '{table_desc}'\n",
    "            TBLPROPERTIES ({tbl_properties})\n",
    "            LOCATION '{path}'\n",
    "        \"\"\")\n",
    "        spark.sql(sql_query)\n",
    "\n",
    "        self.ts_print(f\"Successfully created table `{self.catalog}.{layer}.{table}`.\")\n",
    "\n",
    "    def alter_table_properties(self, layer, table, tbl_properties):\n",
    "        '''\n",
    "        Alter the properties of the table if there is a mismatch between the existing properties and the config.\n",
    "        '''\n",
    "        # Only alter table properties if the table doesn't already have the properties set\n",
    "        existing_properties = spark.sql(f\"DESC TABLE EXTENDED {self.catalog}.{layer}.{table}\").filter(\"col_name = 'Table Properties'\").select(\"data_type\").first()[0]\n",
    "        existing_properties = existing_properties.strip(\"[]\").split(\",\")\n",
    "\n",
    "        for k, v in tbl_properties.items():\n",
    "            if f\"{k}={v}\" not in existing_properties:\n",
    "                self.ts_print(f\"Setting TBLPROPERTY: '{k}'='{v}'\")\n",
    "                sql_query = f\"ALTER TABLE {self.catalog}.{layer}.{table} SET TBLPROPERTIES ('{k}'='{v}')\"\n",
    "                spark.sql(sql_query)\n",
    "    \n",
    "    def update_column_names(self, df, config):\n",
    "        '''\n",
    "        Updates the column names from the bronze names to the silver names.\n",
    "        '''\n",
    "        for row in config:\n",
    "            df = df.withColumnRenamed(row['field_raw'], row['field_silver'])\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def add_metadata_columns(self, df, path, layer, load_type):\n",
    "        '''\n",
    "        Adds the details of the processed files as columns to the dataframe.\n",
    "        '''\n",
    "        # Get metadata details\n",
    "        if \"FULL\" in load_type:\n",
    "            file_details = dbutils.fs.ls(path)[0]\n",
    "            file_path = f.lit(file_details[0])\n",
    "            file_name = f.lit(file_details[1])\n",
    "            file_size = f.lit(file_details[2])\n",
    "            file_mod_time = f.lit(file_details[3])\n",
    "        elif \"STREAM\" in load_type:\n",
    "            file_path = f.col(\"_metadata.file_path\")\n",
    "            file_name = f.col(\"_metadata.file_name\")\n",
    "            file_size = f.col(\"_metadata.file_size\")\n",
    "            file_mod_time = f.col('_metadata.file_modification_time')\n",
    "\n",
    "        # Add metadata columns\n",
    "        if \"FULL\" in load_type and layer == \"bronze\":\n",
    "            df = (df\n",
    "                .withColumn(\"_file_path\", file_path)\n",
    "                .withColumn(\"_file_name\", file_name)\n",
    "                .withColumn(\"_file_size\", file_size)\n",
    "                .withColumn(\"_file_modification_time\", file_mod_time)\n",
    "                .withColumn(\"_loaded_at_utc\", f.current_timestamp())\n",
    "            )\n",
    "        elif \"STREAM\" in load_type and layer == \"bronze\":\n",
    "            df = (df\n",
    "                .withColumn(\"_file_path\", file_path)\n",
    "                .withColumn(\"_file_name\", file_name)\n",
    "                .withColumn(\"_file_size\", file_size)\n",
    "                .withColumn(\"_file_modification_time\", file_mod_time)\n",
    "                .withColumn(\"_loaded_at_utc\", f.current_timestamp())\n",
    "                .withColumn(\"_last_updated_at_utc\", f.current_timestamp())\n",
    "            )\n",
    "        elif \"STREAM\" in load_type and layer == \"silver\":\n",
    "            df = (df\n",
    "                .withColumn(\"_loaded_at_utc\", f.current_timestamp())\n",
    "                .withColumn(\"_last_updated_at_utc\", f.current_timestamp())\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    def update_col_dtypes(self, df, config):\n",
    "        '''\n",
    "        Updates column values according to business logic.\n",
    "        '''\n",
    "        columns = df.columns\n",
    "\n",
    "        for row in config:\n",
    "\n",
    "            if row['field_silver'].startswith('is_') or row['field_silver'].startswith('has_'):\n",
    "                df = df.withColumn(\n",
    "                    row['field_raw'],\n",
    "                    f.when(f.col(row['field_raw']) == True, 'Y').otherwise('N')\n",
    "                )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_primary_keys(self, columns, layer):\n",
    "        '''\n",
    "        Gets the primary keys for the specified layer.\n",
    "        '''\n",
    "        filter_col = \"field_raw\" if layer == \"bronze\" else \"field_silver\"\n",
    "        df = self.config.filter(\n",
    "            (f.col(filter_col).isin(columns)) &\n",
    "            (f.col(\"layer\") == \"bronze\")\n",
    "        )\n",
    "        df = df.collect()\n",
    "\n",
    "        return df[0][\"primary_keys\"]\n",
    "\n",
    "    def write_upsert(self, source_data, target_path, primary_keys, config, layer, load_type):\n",
    "        '''\n",
    "        Upserts records from the target dataframe to the target location.\n",
    "        '''\n",
    "        self.ts_print(\"Executing UPSERT...\")\n",
    "        # De-duplicate records in the case that the same record is present in multiple files in this batch\n",
    "        partition_by_cols = self.get_primary_keys(primary_keys.split(\";\"), layer)\n",
    "        partition = Window().partitionBy(partition_by_cols.split(\";\"))\n",
    "        order_by = f.desc(f.col(f\"_loaded_at_utc\"))\n",
    "\n",
    "        source_data = source_data.withColumn(\"Rank\", f.row_number().over(partition.orderBy(order_by)))\n",
    "        source_data = source_data.filter(\"Rank = 1\").drop(\"Rank\")\n",
    "\n",
    "        join_condition = \"1 = 1\" # 1 = 1 allows us to append all subsequent conditions using AND\n",
    "        for row in config.filter(\"is_primary_key = 'Y'\").collect():\n",
    "            target_key = row[\"field_raw\" if layer == \"bronze\" else \"field_silver\"]\n",
    "            source_key = row[\"field_raw\"]\n",
    "            join_condition += f\" AND target.`{target_key}` = source.`{source_key}`\" # Use ` backtick to safeguard column names that contain spaces\n",
    "\n",
    "        # Specify field mapping for updates and inserts\n",
    "        update_fields = {}\n",
    "        insert_fields = {}\n",
    "        for row in config.collect():\n",
    "            k = row[\"field_raw\" if layer == \"bronze\" else \"field_silver\"]\n",
    "            v = f\"source.{row['field_raw']}\"\n",
    "            update_fields[k] = v\n",
    "            insert_fields[k] = v\n",
    "\n",
    "        insert_fields[\"_loaded_at_utc\"] = \"current_timestamp()\"\n",
    "        insert_fields[\"_last_updated_at_utc\"] = \"current_timestamp()\"\n",
    "        update_fields[\"_last_updated_at_utc\"] = \"current_timestamp()\"\n",
    "\n",
    "        if layer == \"bronze\":\n",
    "            insert_fields[\"_file_path\"] = \"source._file_path\"\n",
    "            insert_fields[\"_file_name\"] = \"source._file_name\"\n",
    "            insert_fields[\"_file_size\"] = \"source._file_size\"\n",
    "            insert_fields[\"_file_modification_time\"] = \"source._file_modification_time\"\n",
    "            insert_fields[\"_loaded_at_utc\"] = \"source._loaded_at_utc\"\n",
    "\n",
    "        if layer == \"bronze\" and \"STREAM\" in load_type:\n",
    "            insert_fields[\"_last_updated_at_utc\"] = \"source._last_updated_at_utc\"\n",
    "\n",
    "        if layer == \"silver\" and \"STREAM\" in load_type:\n",
    "            insert_fields[\"_loaded_at_utc\"] = \"source._loaded_at_utc\"\n",
    "            insert_fields[\"_last_updated_at_utc\"] = \"source._last_updated_at_utc\"\n",
    "\n",
    "        # Execute merge\n",
    "        target_table = DeltaTable.forPath(spark, target_path)\n",
    "        (\n",
    "            target_table.alias(\"target\")\n",
    "            .merge(source_data.alias(\"source\"), join_condition)\n",
    "            .whenMatchedUpdate(set=update_fields)\n",
    "            .whenNotMatchedInsert(values=insert_fields)\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    def write_bronze(self):\n",
    "        '''\n",
    "        Writes the raw data to the bronze container and creates a table in the bronze schema.\n",
    "        '''\n",
    "        self.ts_print(f\"⏩ Executing bronze layer ingestion...\")\n",
    "\n",
    "        # Get config\n",
    "        config_df = self.config.filter(\"layer = 'bronze'\")\n",
    "        config = config_df.collect()\n",
    "\n",
    "        # Set variables\n",
    "        table = config[0]['table']\n",
    "        source_table = config[0]['source_table']\n",
    "        file_type = (config[0]['load_type']).split(\" \")[-1].lower()\n",
    "        raw_path = f\"{self.raw_path}/{source_table}\"\n",
    "        bronze_path =f\"{self.bronze_path}/{table}\"\n",
    "        table_desc = config[0]['table_description']\n",
    "        primary_keys = config[0]['primary_keys']\n",
    "        load_type = config[0]['load_type']\n",
    "        schema = self.get_schema('bronze', load_type)\n",
    "        is_cdf = config[0]['is_cdf']\n",
    "\n",
    "        # Set table properties\n",
    "        tbl_properties = {}\n",
    "        tbl_properties[\"delta.enableChangeDataFeed\"] = \"true\" if is_cdf == \"Y\" else \"false\"\n",
    "        tbl_properties[\"primary_keys\"] = primary_keys\n",
    "\n",
    "        # Full load to overwrite\n",
    "        if load_type.startswith(\"FULL\"):\n",
    "\n",
    "            self.ts_print(\"Initiating full load...\")\n",
    "\n",
    "            # Read data from raw\n",
    "            df = (\n",
    "                spark.read\n",
    "                .format(file_type)\n",
    "                .option(\"header\", True)\n",
    "                .schema(schema)\n",
    "                .load(raw_path)\n",
    "            )\n",
    "\n",
    "            # Update column values according to business logic\n",
    "            df = self.update_col_dtypes(df, config)\n",
    "\n",
    "            # Add metadata processing columns\n",
    "            df = self.add_metadata_columns(df, raw_path, 'bronze', load_type)\n",
    "\n",
    "            # Create the table and external location if they don't exist\n",
    "            if not self.table_exists('bronze', table):\n",
    "                self.create_table('bronze', table, table_desc, bronze_path, tbl_properties, config, load_type)\n",
    "\n",
    "            # Alter table properties that don't match the config\n",
    "            self.alter_table_properties(\"bronze\", table, tbl_properties)\n",
    "\n",
    "            # Write data to bronze\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).save(bronze_path)\n",
    "\n",
    "            self.ts_print(f\"Raw files from {raw_path} successfully written to {bronze_path}.\")\n",
    "        \n",
    "        # Incremental load for UPSERT\n",
    "        elif load_type.startswith(\"STREAM\"):\n",
    "\n",
    "            self.ts_print(\"Initiating stream...\")\n",
    "\n",
    "            # Create the table and external location if they don't exist\n",
    "            if not self.table_exists('bronze', table):\n",
    "                self.create_table('bronze', table, table_desc, bronze_path, tbl_properties, config, load_type)\n",
    "\n",
    "            # Alter table properties that don't match the config\n",
    "            self.alter_table_properties(\"bronze\", table, tbl_properties)\n",
    "\n",
    "            # Read unprocessed files from raw\n",
    "            df = (\n",
    "                spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", file_type)\n",
    "                .option(\"cloudFiles.useIncrementalListing\", True) # Read files alphabetically (yyyMMdd_HHmmss format)\n",
    "                .option(\"cloudFiles.schemaLocation\", f\"{bronze_path}/_schema\")\n",
    "                .option(\"cloudFiles.backfillInterval\", \"1 day\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"overwriteSchema\", False)\n",
    "                .option(\"mergeSchema\", False)\n",
    "                .schema(schema)\n",
    "                .load(raw_path)\n",
    "            )\n",
    "            \n",
    "            # Add metadata processing columns\n",
    "            df = self.add_metadata_columns(df, raw_path, 'bronze', load_type)\n",
    "\n",
    "            # Execute UPSERT for all batches of changes\n",
    "            micro_batch = lambda batch_df, batch_id: self.write_upsert(batch_df, bronze_path, primary_keys, config_df, \"bronze\", load_type)\n",
    "            query = (\n",
    "                df.writeStream\n",
    "                .foreachBatch(micro_batch)\n",
    "                .option(\"checkpointLocation\", f\"{bronze_path}/_checkpoint\")\n",
    "                .trigger(once=True)\n",
    "                .start(bronze_path)\n",
    "            )\n",
    "\n",
    "            # Wait for the stream to complete before continuing because it is asynchronous\n",
    "            query.awaitTermination()\n",
    "\n",
    "            self.ts_print(f\"Raw files from {raw_path} successfully written to {bronze_path}.\")\n",
    "\n",
    "        self.ts_print(f\"⏩ Bronze layer ingestion complete.\")\n",
    "        print('')\n",
    "    \n",
    "    def write_silver(self):\n",
    "        '''\n",
    "        Writes the bronze data to the silver container and creates a table in the silver schema.\n",
    "        '''\n",
    "        self.ts_print(f\"⏩ Executing silver layer ingestion...\")\n",
    "\n",
    "        # Set config\n",
    "        config_df = self.config.filter(\"layer = 'silver'\")\n",
    "        config = config_df.collect()\n",
    "\n",
    "        # Set variables\n",
    "        table = config[0]['table']\n",
    "        source_table = config[0]['source_table']\n",
    "        table_desc = config[0]['table_description']\n",
    "        bronze_path = f\"{self.bronze_path}/{source_table}\"\n",
    "        silver_path = f\"{self.silver_path}/{table}\"\n",
    "        load_type = config[0]['load_type']\n",
    "        schema = self.get_schema('silver', load_type)\n",
    "        primary_keys = config[0]['primary_keys']\n",
    "\n",
    "        # Set table properties\n",
    "        tbl_properties = {}\n",
    "        tbl_properties[\"primary_keys\"] = primary_keys\n",
    "\n",
    "        # Full load to overwrite\n",
    "        if load_type.startswith(\"FULL\"):\n",
    "\n",
    "            self.ts_print(\"Initiating full load...\")\n",
    "\n",
    "            # Read data from bronze table\n",
    "            df = (\n",
    "                spark.read\n",
    "                .option(\"header\", True)\n",
    "                .table(f\"{self.catalog}.bronze.{source_table}\")\n",
    "            )\n",
    "\n",
    "            # Drop processing columns\n",
    "            for col in df.columns:\n",
    "                if col.startswith('_'):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "            # Update column names to silver names\n",
    "            df = self.update_column_names(df, config)\n",
    "\n",
    "            # Create the table and external location if they don't exist\n",
    "            if not self.table_exists('silver', table):\n",
    "                self.create_table('silver', table, table_desc, silver_path, tbl_properties, config, load_type)\n",
    "\n",
    "            # Alter table properties that don't match the config\n",
    "            self.alter_table_properties(\"silver\", table, tbl_properties)\n",
    "\n",
    "            # Write data to silver\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).save(silver_path)\n",
    "\n",
    "        # Incremental load for CDF\n",
    "        elif load_type.startswith(\"STREAM\"):\n",
    "            \n",
    "            self.ts_print(\"Initiating stream...\")\n",
    "\n",
    "            # Create the table and external location if they don't exist\n",
    "            if not self.table_exists('silver', table):\n",
    "                self.create_table('silver', table, table_desc, silver_path, tbl_properties, config, load_type)\n",
    "\n",
    "            # Alter table properties that don't match the config\n",
    "            self.alter_table_properties(\"silver\", table, tbl_properties)\n",
    "\n",
    "            # Read bronze data via change data feed\n",
    "            df = (\n",
    "                spark.readStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"readChangeFeed\", \"true\")\n",
    "                .option(\"startingVersion\", 0)\n",
    "                .table(f\"{self.catalog}.bronze.{source_table}\")\n",
    "            )\n",
    "\n",
    "            # Execute UPSERT for all batches of changes\n",
    "            micro_batch = lambda batch_df, batch_id: self.write_upsert(batch_df, silver_path, primary_keys, config_df, \"silver\", load_type)\n",
    "            query = (\n",
    "                df.writeStream\n",
    "                .foreachBatch(micro_batch)\n",
    "                .option(\"checkpointLocation\", f\"{silver_path}/_checkpoint\")\n",
    "                .trigger(once=True)\n",
    "                .start(silver_path)\n",
    "            )\n",
    "\n",
    "            # Wait for the stream to complete before contuining because it is asynchronous\n",
    "            query.awaitTermination()\n",
    "\n",
    "        self.ts_print(f\"Bronze data from {bronze_path} successfully written to {silver_path}.\")\n",
    "\n",
    "        self.ts_print(f\"⏩ Silver layer ingestion complete.\")\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
